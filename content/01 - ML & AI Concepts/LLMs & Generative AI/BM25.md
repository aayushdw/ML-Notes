**BM25** scores how well a document matches a query. It's more sophisticated than just counting word frequency.
## BM25 Formula Explained
**The Formula**:
$$BM25(D, Q) = \sum_{q \in Q} IDF(q) \cdot \frac{TF(q, D) \cdot (k_1 + 1)}{TF(q, D) + k_1 \cdot (1 - b + b \cdot \frac{|D|}{avgdl})}$$
This looks intimidating, but it has three simple parts.

---
## Part 1: IDF (Inverse Document Frequency)

**What it does**: Measures how rare/important a word is.
$$IDF(q) = \log\left(\frac{N - n_q + 0.5}{n_q + 0.5}\right)$$
Where:
- $N$ = total documents in corpus
- $n_q$ = documents containing word $q$
**Example**:
- Word "the" appears in 1,000,000 out of 1,000,000 docs → IDF ≈ -7 (useless, penalized)
- Word "transformer" appears in 50 out of 1,000,000 docs → IDF ≈ 9.9 (very useful, boosted)

**Intuition**: Common words (the, and, is) get low or negative scores. Rare, meaningful words get high scores.

---

## Part 2: TF Component (Term Frequency with Saturation)

**What it does**: Counts how many times the word appears in the document, but with diminishing returns.
$$\frac{TF(q, D) \cdot (k_1 + 1)}{TF(q, D) + k_1}$$
Where:
- $TF(q, D)$ = count of word $q$ in document $D$
- $k_1$ = saturation parameter (default 1.2)

**Why saturation?** If a doc mentions "AI" 1 time vs 100 times, the 100-time doc isn't 100x more relevant. Repetition has diminishing returns.

**Example with $k_1 = 1.2$**:

| TF Count | Formula                                 | Result    | Interpretation              |
| -------- | --------------------------------------- | --------- | --------------------------- |
| 1        | (1 × 2.2) / (1 + 1.2) = 2.2 / 2.2       | **1.0**   | First mention: full credit  |
| 2        | (2 × 2.2) / (2 + 1.2) = 4.4 / 3.2       | **1.375** | Second mention: less credit |
| 5        | (5 × 2.2) / (5 + 1.2) = 11 / 6.2        | **1.77**  | Plateau starting            |
| 10       | (10 × 2.2) / (10 + 1.2) = 22 / 11.2     | **1.96**  | Approaching ceiling         |
| 50       | (50 × 2.2) / (50 + 1.2) = 110 / 51.2    | **2.15**  | Almost maxed out            |
| 100      | (100 × 2.2) / (100 + 1.2) = 220 / 101.2 | **2.17**  | Ceiling reached             |

**See the pattern?** Score rises quickly at first, then plateaus. The maximum possible score is $k_1 + 1 = 2.2$.

---

## Part 3: Length Normalization

**What it does**: Fairness adjustment so long documents don't always win.
$$1 - b + b \cdot \frac{|D|}{avgdl}$$
Where:
- $b$ = normalization strength (default 0.75)
- $|D|$ = length of document (word count)
- $avgdl$ = average document length in corpus

**Why?** A long document naturally has more word matches. But a short article specifically *about* a topic is more relevant than a passing mention in a textbook.

**Example with $b = 0.75$, $avgdl = 200$**:

| Doc Length | Calculation | Result | Effect |
|------------|-------------|--------|--------|
| 50 words (short) | 1 - 0.75 + 0.75 × (50/200) | **0.4375** | Gets boosted (divide by smaller number) |
| 100 words | 1 - 0.75 + 0.75 × (100/200) | **0.625** | Slight boost |
| 200 words (avg) | 1 - 0.75 + 0.75 × (200/200) | **1.0** | Baseline (no change) |
| 500 words | 1 - 0.75 + 0.75 × (500/200) | **2.125** | Gets penalized (divide by larger number) |
| 1000 words (long) | 1 - 0.75 + 0.75 × (1000/200) | **3.75** | Heavily penalized |

---
## Putting It All Together

**Full TF+Length formula** (the denominator):
$$TF(q, D) + k_1 \cdot \left(1 - b + b \cdot \frac{|D|}{avgdl}\right)$$
This combines:
- **Saturation**: TF stops growing fast
- **Length penalty**: Longer docs get bigger denominators

**Final calculation for each query word**:
$$IDF(q) \cdot \frac{TF(q, D) \cdot (k_1 + 1)}{TF(q, D) + k_1 \cdot (1 - b + b \cdot \frac{|D|}{avgdl})}$$

**Then sum** across all query words to get the total BM25 score.

---

## Complete Walkthrough Example

**Setup**:
- Total documents: 10,000
- Average doc length: 200 words
- Query: "Python tutorial"
- Doc A: 150 words, contains "Python" 3 times, "tutorial" 2 times
- Doc B: 800 words, contains "Python" 1 time, "tutorial" 1 time
- Parameters: $k_1 = 1.2$, $b = 0.75$

**Step 1: Calculate IDF**
- "Python" in 100 docs → IDF = log((10,000-100+0.5)/(100+0.5)) ≈ **4.57**
- "tutorial" in 500 docs → IDF = log((10,000-500+0.5)/(500+0.5)) ≈ **2.96**

**Step 2: BM25 for Doc A (150 words)**
- Length factor: 1 - 0.75 + 0.75 × (150/200) = **0.8125**

For "Python" (TF=3):
- TF component: (3 × 2.2) / (3 + 1.2 × 0.8125) = 6.6 / 3.975 = **1.66**
- Contribution: 4.57 × 1.66 = **7.59**

For "tutorial" (TF=2):
- TF component: (2 × 2.2) / (2 + 1.2 × 0.8125) = 4.4 / 2.975 = **1.48**
- Contribution: 2.96 × 1.48 = **4.38**

**Doc A Total: 7.59 + 4.38 = 11.97**

**Step 3: BM25 for Doc B (800 words)**
- Length factor: 1 - 0.75 + 0.75 × (800/200) = **3.75**

For "Python" (TF=1):
- TF component: (1 × 2.2) / (1 + 1.2 × 3.75) = 2.2 / 5.5 = **0.4**
- Contribution: 4.57 × 0.4 = **1.83**

For "tutorial" (TF=1):
- TF component: (1 × 2.2) / (1 + 1.2 × 3.75) = 2.2 / 5.5 = **0.4**
- Contribution: 2.96 × 0.4 = **1.18**

**Doc B Total: 1.83 + 1.18 = 3.01**

**Result**: Doc A (11.97) ranks much higher than Doc B (3.01) ✓

Why? Doc A is shorter and more focused on the topic. Doc B is long and dilutes the matches.

---

## Parameter Tuning Guide

| Parameter | Default | Range | Effect |
|-----------|---------|-------|--------|
| **$k_1$** | 1.2 | 0.5 - 2.0 | Lower = ignore repetition; Higher = reward repetition more |
| **$b$** | 0.75 | 0.0 - 1.0 | 0 = ignore length; 1.0 = heavily penalize long docs |

**Common values**:
- $k_1 = 1.2, b = 0.75$ → Default (balanced)
- $k_1 = 2.0, b = 0.75$ → More repetition weight (for verbose docs)
- $k_1 = 1.2, b = 0.0$ → Ignore document length (when length is irrelevant)

